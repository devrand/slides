# Apache Drill

— Що таке Apache Drill?

— Це інструмент для того щоб працювати з великими JSON / CSV файлами (і ще низкою форматів для великих даних, таких як Parquet). 

Для операцій типу простого SELECT (вибрати якісь колонки з csv-файлу) його швидкість в 10 разів перевищує такий інструмент як csvkit, і близька до швидкості потокових процесорів  shell, типу awk. 

Тестовий час для того щоб вибрати чотири колонки з csv-файлу з 1.1 мільйоном строк, розміром в ~180 МБт був близько  3 секунд на моїй машині( так само для awk, натомість для csvcut час був ~40 с)
 
І це для одномашинної, локальної версії, на неоптимізованих текстових файлах. Є ще варіант запускати Drill-кластери на різних компах, тоді швидкість буде значно більша.  

Але це не головне - у Drill є те, чого немає у інструментів командної строки - він є SQL системою без необхідності створення схеми для БД, який використовує вже існуючі текстові файли і перетворює їх "на льоту" у базу даних!

Наприклад, робота з навіть дуже складним JSON [значно спрощується/прискорюється](https://www.rittmanmead.com/blog/2016/08/an-introduction-to-apache-drill/) 

Крім SQL запитів до одного файлу, можна зробити БД з набору файлів, із директорії з файлами, або з ієрархічного дерева директорій.

Бонус - текстові файли можуть бути у вигляді архівів!!!


Офіційно:
Drill - швидкий, розподілений query engine  для роботи як з реляційними, так і NOSQL сховищами даних, з можливістю обробки петабайтів даних. Користувачі можуть робити запити, використовуючи SQL або засоби BI-інструментів (через ODBC інтерфейси), не переймаючись створенням та підтримкою схем баз даних.



# Ключові властивості:

* Безсхемна модель, схожа на MongoDB та Elasticsearch
* Стандартне та звичне API: ANSI SQL, ODBC/JDBC, RESTful APIs
* Дружній до користувачів та розробників
* Універсальний конектор/зклеювач - архітектура дозволяє з'єднуватися з багатьма типами сховищ даних

Розподілене середовище: на кожній машині запускається "Drillbit"-процес для комунікації з клієнтами, ZooKeeper для координації вузлів кластеру




# Інсталяція
Вона нескладна. Для різних систем можна прочитати коротку [інструкцію тут](https://drill.apache.org/docs/drill-in-10-minutes/)

Увага! Треба перевірити, чи встановлена у вас Java та JDK (однієї JRE недостатньо, буде видавати помилку після запуску). Інколи потрібно прослідкувати щоб у JAVA_HOME був прописаний коректний шлях на вашій системі, і в PATH має бути  $JAVA_HOME/bin  


# Запити до CSV
ПІсля інсталяції все готово для того, щоб робити запити. Запуск bin/drill-embedded в директорії інсталяції дає доступ до drill shell, де можна писати SQL. 

[[Необов"язково]] Для зручності роботи з CSV файлами потрібно лише трохи змінити конфігурацію (це можна зробити прямо в браузері, за адресою http://localhost:8047, потім вибрати "storage" потім "dfs") і виправити фрагмент 
```{json}
"csv": {
      "type": "text",
      "extensions": [
        "csv"
      ],
      "delimiter": ","
    }, 
```
на 

```{json}
"csv": {
      "type": "text",
      "extensions": [
        "csv"
      ],
      "skipFirstLine": true,
      "extractHeader": true,
      "delimiter": ","
    },
```
Це змусить Drill вважати першу строку з хедером як власне хедер з назвами колонок, і дозволить використовувати ваші назви колонок в SQL-запитах. 



[[ Необов"язково ]], але для того, щоб зберігти ваші зміни між запусками, треба зробити наступне)
mv /tmp/drill /my/configpath перемістити тимчасовий файл з конфігом у зручну для вас директорію
# у файлі drill-override.conf з директорії conf зробити такі зміни 
drill.exec: {sys.store.provider.local.path="/my/configpath"}

[[ Необов"язково ]], але можна перетворити CSV файл у Parquet формат, тоді запити будуть виконуватися значно швидше!)
CREATE TABLE dfs.pq.`/ваш/новий/файл.parquet` AS 
  SELECT * FROM dfs.csv.`/ваші/старі_файли_один_або_багато/*.csv.bz2`


Тепер про власне запити, командне запрошення у shell виглядає так 
```{SQL}
0: jdbc:drill:zk=local>

/* пишемо звичайний SQL: */

SELECT * FROM dfs.`/full/path/to/you/csv/file/2016-07-20_w_header.csv`;

/* dfs - тип сховища, файлова система. Якщо не використовується воркспейс (див нижче), потрібно вказати повний шлях до файла, якщо файлів у директорії багато, можна і так: */

SELECT * FROM dfs.`/full/path/to/you/csv/files/sample-data/*.csv.bz2`;

/* якщо ви не зробили зміни у конфігурації по хедеру, до колонок у запиті можна звертатися так: */


SELECT columns[0] as id, columns[1] as speed, columns[5] as five  FROM dfs.`/full/path/to/you/csv/file/sample-data/2016-07-20_w_header.csv`;
```
```{json}
/* Класно конфігурувати окремі воркспейси для роботи з різними даними: */

"workspaces": {
    "csv": {
      "location": "/full/path/to/you/csv/files/csv",
      "writable": true,
      "defaultInputFormat": "csv"
    },
    "pq": {
      "location": "/full/path/to/you/parquet/files/pq",
      "writable": true,
      "defaultInputFormat": "parquet"
    },
    ...
```
```{SQL}
/* Після цього робити запити значно зручніше - Реальний приклад збереження CSV у Parquet */
CREATE TABLE dfs.pq.`data.parquet` AS  /* вже використано воркспейс dfs.pq */
	SELECT * FROM dfs.csv.`2016-07-20_w_header.csv`; /* та dfs.csv з конфігурації вверху*/


/* Після перетворення у Parquet-формат для цього запиту швидкість зросла у 6 разів: */
select avg(  cast(SPEED as float) )  from dfs.csv.`2016-07-20_w_header.csv`; /* 1 row selected (2.057 seconds) */

select avg(  cast(SPEED as float) )  from dfs.pq.`data.parquet`;  /* 1 row selected (0.375 seconds) */  
```

Інші бенчмарки, наприклад  у цій корисній [статті Боба Рудіса](https://rud.is/b/2017/05/31/drilling-into-csvs-teaser-trailer/) говорять що швидкість роботи з CSV файлами у R через Drill (бібліотека sergeant) співставна по швидкості з роботою з Rda файлами, і в 20 разів швидше (ніж Rda) якщо ми використовуємо Parquet!


# Висновок

Отже, Drill є потужним "швейцарським ножиком" для роботи з великими файлами у різноманітних форматах даних / базах даних.
Він "склеює" всі ці джерела та автоматично додає можливість робити SQL запити навіть там, де SQL-схема відсутня. Скейлиться до петабайт даних (якщо є можливість додати до кластера тисячі машин, ггг).

* [Використання з R, продовження цього матеріалу](https://github.com/devrand/texty_drill_intro/blob/master/drill_tutorial_sergeant.Rmd)
* [Використання з R - (Боб Рудіс)](https://github.com/hrbrmstr/sergeant)
* [Приклад аналізу для R](https://rud.is/rpubs/yelp.html)
* [вступна стаття](https://rud.is/b/2017/05/31/drilling-into-csvs-teaser-trailer/)

* [Використання з Python - pydrill](https://github.com/PythonicNinja/pydrill)
* [Приклад аналізу для Python](https://github.com/psychemedia/parlihacks/blob/master/notebooks/Apache%20Drill%20-%20JSON%20Written%20Questions.ipynb)
